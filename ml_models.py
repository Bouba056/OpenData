# ================================================================
# ü§ñ MODULE ML ‚Äî Version robuste et professionnelle (2025)
# ================================================================
# Ce module regroupe :
# 1. Clustering automatique optimis√© (Silhouette + KMeans)
# 2. Score de tension immobili√®re bas√© sur PCA + pond√©ration
# 3. Pr√©diction du nombre de logements (lin√©aire / exponentielle)
# ================================================================

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from scipy.optimize import curve_fit
import streamlit as st


# =================================================================
# üîµ 1) CLUSTERING ‚Äî Profils de communes
# =================================================================

@st.cache_data
def identifier_profils_communes(data, k_max=5):
    """
    Clustering automatique bas√© sur KMeans + Silhouette.
    """
    
    df = data.copy().fillna(0)

    # ======= AJOUT ESSENTIEL POUR TON CAS =======
    # =============================================

    variables = [
        "Plog_RP",
        "Plog_RS",
        "Plog_VAC",
        "Plog_MAISON",
        "Plog_APPART"
    ]

    df = data.copy().fillna(0)
    df["Plog_MAISON"] = (df["MAISON"] / df["LOG"]) * 100
    df["Plog_APPART"] = (df["APPART"] / df["LOG"]) * 100

    X = df[variables]

    # Normalisation
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # S√©lection automatique du meilleur nombre de clusters (2 ‚Üí k_max)
    best_k = 2
    best_sil = -1

    for k in range(2, k_max + 1):
        model = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = model.fit_predict(X_scaled)
        sil = silhouette_score(X_scaled, labels)
        if sil > best_sil:
            best_sil = sil
            best_k = k

    # Clustering final
    final = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    profils = final.fit_predict(X_scaled)

    df["Profil"] = profils

    global_means = df[variables].mean()

    label_for_var = {
        "Plog_RP": "R√©sidences principales",
        "Plog_RS": "R√©sidences secondaires",
        "Plog_VAC": "Vacance",
        "Plog_MAISON": "Part de maisons",
        "Plog_APPART": "Part d'appartements",
    }

    label_for_name = {
        "Plog_RP": ("r√©sidences principales √©lev√©es", "r√©sidences principales faibles"),
        "Plog_RS": ("r√©sidences secondaires √©lev√©es", "peu de r√©sidences secondaires"),
        "Plog_VAC": ("vacance marqu√©e", "vacance limit√©e"),
        "Plog_MAISON": ("dominance des maisons", "faible part de maisons"),
        "Plog_APPART": ("dominance des appartements", "faible part d'appartements"),
    }

    descriptions = {}
    total_communes = len(df)

    for p in range(best_k):
        sous_df = df[df["Profil"] == p]
        stats = sous_df[variables].mean()

        deltas = {var: stats[var] - global_means[var] for var in variables}
        insights = []

        part = (len(sous_df) / total_communes) * 100 if total_communes else 0
        insights.append(f"{len(sous_df)} communes ({part:.1f}% de l'√©chantillon)")

        for var, label in label_for_var.items():
            delta = deltas[var]
            if np.isnan(delta):
                continue
            if delta >= 5:
                insights.append(f"{label} sup√©rieures √† la moyenne ({stats[var]:.1f}% ; {delta:.1f} pts)")
            elif delta <= -5:
                insights.append(f"{label} inf√©rieures √† la moyenne ({stats[var]:.1f}% ; {delta:.1f} pts)")

        if "DEP" in sous_df.columns:
            deps = sous_df["DEP"].astype(str).value_counts().head(2)
            if not deps.empty:
                dep_txt = ", ".join([
                    f"{dep} ({count / len(sous_df) * 100:.0f}%".rstrip("0").rstrip(".") + "%)"
                    for dep, count in deps.items()
                ])
                insights.append(f"R√©partition des d√©partements : {dep_txt}")

        significant_deltas = sorted(deltas.items(), key=lambda kv: abs(kv[1]), reverse=True)
        suffix = "profil √©quilibr√©"
        for var, delta in significant_deltas:
            if abs(delta) >= 3:
                name_options = label_for_name.get(var)
                if name_options:
                    suffix = name_options[0] if delta >= 0 else name_options[1]
                    break

        if suffix == "profil √©quilibr√©":
            profil_name = f"Profil {p+1}"
        else:
            profil_name = f"Profil {p+1} ‚Äì {suffix}"

        detail_points = [txt for txt in insights[1:] if "%" in txt]
        description = " ‚Ä¢ ".join(detail_points[:2]) if detail_points else "Profil √©quilibr√©"

        descriptions[p] = {
            "nom": profil_name,
            "description": description,
            "insights": insights[:5]
        }

    df["Nom_Profil"] = df["Profil"].map(lambda x: descriptions[x]["nom"])

    return df, descriptions


# =================================================================
# üîµ 2) SCORE DE TENSION IMMOBILI√àRE (M√©thode PCA pond√©r√©e)
# =================================================================

@st.cache_data
def calculer_tension_immobiliere(data):
    """
    Score de tension robuste bas√© sur :
    - Standardisation
    - PCA pour pond√©rer objectivement les variables
    - Score normalis√© entre 0 et 100

    Variables utilis√©es :
        - Vacance (%)
        - R√©sidences secondaires (%)
        - Propri√©taires (%)

    Retourne DF avec Score_Tension et Niveau.
    """

    df = data.copy().fillna(0)
    variables = ["Plog_VAC", "Plog_RS", "Prp_RP_PROP"]

    X = df[variables]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # PCA pour pond√©ration objective
    pca = PCA(n_components=1)
    composante = pca.fit_transform(X_scaled).flatten()

    # Normalisation en score 0-100
    score = (composante - composante.min()) / (composante.max() - composante.min())
    score = score * 100

    df["Score_Tension"] = score.round(1)

    df["Niveau"] = pd.cut(
        df["Score_Tension"],
        bins=[0, 25, 50, 75, 100],
        labels=["üü¢ Faible", "üü° Mod√©r√©e", "üü† √âlev√©e", "üî¥ Tr√®s √©lev√©e"],
        include_lowest=True
    )

    return df


# =================================================================
# üîµ 3) PR√âDICTION DU PARC ‚Äî Lin√©aire ou exponentielle
# =================================================================

def _exp_model(x, a, b):
    """Mod√®le exponentiel simple : y = a * exp(bx)"""
    return a * np.exp(b * x)


@st.cache_data
def predire_evolution_logements(data, commune, annees_futures=3):
    """
    Pr√©dit le nombre de logements d'une commune via :
    - R√©gression lin√©aire
    - R√©gression exponentielle
    S√©lection automatique du meilleur mod√®le selon le RMSE.

    Retour :
      predictions (DataFrame)
      croissance annuelle (%)
    """

    df = data[data["LIBGEO"] == commune].sort_values("AN")
    if len(df) < 3:
        return None, None

    X = df["AN"].values
    y = df["LOG"].values
    X_reshape = X.reshape(-1, 1)

    # -------------------------
    # üîπ Mod√®le LIN√âAIRE
    # -------------------------
    lin = LinearRegression()
    lin.fit(X_reshape, y)
    y_pred_lin = lin.predict(X_reshape)
    rmse_lin = np.sqrt(((y - y_pred_lin) ** 2).mean())

    # -------------------------
    # üîπ Mod√®le EXPONENTIEL
    # -------------------------
    try:
        params, _ = curve_fit(_exp_model, X, y, maxfev=10000)
        y_pred_exp = _exp_model(X, params[0], params[1])
        rmse_exp = np.sqrt(((y - y_pred_exp) ** 2).mean())
    except:
        rmse_exp = np.inf

    # -------------------------
    # üîπ CHOIX AUTOMATIQUE
    # -------------------------
    if rmse_lin <= rmse_exp:
        model_used = "lin√©aire"
        future_years = np.arange(X.max() + 1, X.max() + annees_futures + 1)
        future_pred = lin.predict(future_years.reshape(-1, 1))
    else:
        model_used = "exponentiel"
        future_years = np.arange(X.max() + 1, X.max() + annees_futures + 1)
        future_pred = _exp_model(future_years, params[0], params[1])

    # -------------------------
    # üîπ DATAFRAME FINAL
    # -------------------------
    df_hist = pd.DataFrame({
        "Ann√©e": X,
        "Logements": y,
        "Type": "Historique"
    })

    df_pred = pd.DataFrame({
        "Ann√©e": future_years,
        "Logements": future_pred,
        "Type": "Pr√©diction"
    })

    full = pd.concat([df_hist, df_pred], ignore_index=True)
    croissance = ((future_pred[-1] - y[-1]) / y[-1]) * 100 / annees_futures

    return full, croissance



def get_stats_profil(df, profil_id):
    """
    Retourne les statistiques simples d'un profil :
    - nombre de communes
    - moyenne LOG, RP, VAC, etc.
    """
    subset = df[df["Profil"] == profil_id]

    return {
        "count": len(subset),
        "log_mean": subset["LOG"].mean(),
        "rp_mean": subset["RP"].mean(),
        "vac_mean": subset["LOGVAC"].mean()
    }
